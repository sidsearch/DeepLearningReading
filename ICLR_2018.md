# Generative Models
* Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models
  
  what it does:
  
  condition generation without retraining the model. By post-hoc learning latent constraints, value functions identify regions in latent space that generate outputs with desired attributes, we can conditionally sample from these regions with gradient-based optimization or amortized actor functions. Combining attribute constraints with a universal “realism” constraint, which enforces similarity to the data distribution, we generate realistic conditional images from an unconditional variational autoencoder

* Stochastic Variational Video Prediction

* Variational Continual Learning

* Stochastic Gradient Descent Performs variational inference, converges to limit cycles for deep networks

* Kernel Implicit Variational Inference

* Variational image compression with a scale hyperprior
