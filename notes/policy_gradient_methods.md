methods: that learn the value of state/action 
value function approx: V_theta(S) = V_pi(S)
			Q_theta(S) = Q_pi(S)
parametrized policy:
	policy weight theta
	polciy gradient methods
	unbiased estimator
actor-critic methods??

advantage: ??, high dim space, stochastic polciy?

policy gradient theorem(episodic)

perf measure: 
