Summary:
What is this paper about?
The authors present a system to automatically generate natural language descriptions from images that exploits both statistics gleaned from parsing large quantities of text data and recognition algorithms from computer vision. The system is very effective at producing relevant sentences for images.

What is the main contribution?
This paper presents a system for generating natural language descriptions of images. It  detects objects, modifiers (adjectives), and spatial relationships(prepositions), in an image, smoothing these detections with respect to a statistical prior obtained from descriptive text. Then it uses the smoothed results as constraints for sentence generation.

Describe the main approach & results. Just facts, no opinions yet.

	These are the steps in system:
	  1. object and stuff detectors find candidate objects
	  2. Then each candidate region is processed by a set of attribute classifiers
	  3. each pair of candidate regions is processed by prepositional relationship functions
	  4. Then a CRF is constructed that incorporates the unary image potentials computed in the previous steps
	  5. A labeling of the graph is predicted
	  6. Sentences are generated based on the labeling
	The method achieves really high accuracy in terms of BLEU scores.

	The authors also perform qualitative evaluation and show the natural language descriptions are well generated.

Strengths:
This paper is very well written and easy to understand.
This work laid out some of the foundations for image captioning task which has been followed by many other papers in this area.

Weaknesses:
There has been many other works after this paper which improve upon this work such "show and tell neural caption generator". 
One of the main weakness using template-based text generation. This leads to very rigid description of text generated by the model.

This model is also not really end to end but this weakness has been further addressed.
Reflections:

This is related to some of the papers which read in the beginning of course about image captioning such as "Show and Tell: A Neural Image Caption Generator", "Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning", "Context-aware Captions from Context-agnostic Supervision" etc. 


Most interesting thought:

I think image captioning is solved but not fully solved. There are further challenges building end to end models which somewhat attacked in "DenseCap: Fully Convolutional Localization Networks for Dense Captioning". But it is not clear if these models fully understand picture or not. I think there needs to more work where the image captioning models are trained with less data to understand if these models are compositional or not in nature.